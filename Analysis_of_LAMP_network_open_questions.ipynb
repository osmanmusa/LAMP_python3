{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analysis of LAMP network - open questions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmanmusa/LAMP_python3/blob/master/Analysis_of_LAMP_network_open_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk9sCG2gM0Eq",
        "colab_type": "code",
        "outputId": "df47d834-6218-410b-dea3-d2a48df28877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Clone the git repository by running the following line\n",
        "! git clone https://github.com/osmanmusa/LAMP_python3.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'LAMP_python3'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 62 (delta 16), reused 57 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (62/62), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0Q72oqW61Ic",
        "colab_type": "code",
        "outputId": "e8b19d13-e10a-4a17-cdc7-b7e90fd110dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Copy the LAMP_bg_giid.npz file to your google drive and mount the drive into colab.\n",
        "# Run the following code ...\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# On the left side of your screen, just under \"+ Code\" there is a \">\" button.\n",
        "# When you press this button you will be able to see the list of files folders,\n",
        "# which you can access in colab"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViGq1VgAM9vJ",
        "colab_type": "code",
        "outputId": "5f23a984-b553-4335-b46d-7141d433dd8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Open the LAMP_python3/LAMP.py file and make sure that untied=True when building \n",
        "# the network, and that the learned network is read from the file \n",
        "# /content/drive/My Drive/LAMP_bg_giid.npz, when training  and evaluating the network\n",
        "% run LAMP_python3/LAMP.py\n",
        "# If everything goes well, you should see the following\n",
        "\n",
        "# LAMP-soft T=1 nmse=-6.218525 dB\n",
        "# LAMP-soft linear T=2 nmse=-1.558928 dB\n",
        "# LAMP-soft non-linear T=2 nmse=-11.149492 dB\n",
        "# LAMP-soft linear T=3 nmse=-6.030330 dB\n",
        "# LAMP-soft non-linear T=3 nmse=-15.739241 dB\n",
        "# LAMP-soft linear T=4 nmse=-11.111426 dB\n",
        "# LAMP-soft non-linear T=4 nmse=-20.009360 dB\n",
        "# LAMP-soft linear T=5 nmse=-15.367776 dB\n",
        "# LAMP-soft non-linear T=5 nmse=-24.644411 dB\n",
        "# LAMP-soft linear T=6 nmse=-18.936634 dB\n",
        "# LAMP-soft non-linear T=6 nmse=-29.489040 dB\n",
        "# LAMP-soft linear T=7 nmse=-22.657232 dB\n",
        "# LAMP-soft non-linear T=7 nmse=-33.414526 dB\n",
        "# LAMP-soft linear T=8 nmse=-30.440512 dB\n",
        "# LAMP-soft non-linear T=8 nmse=-37.886918 dB\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/LAMP_python3/LAMP.py:19: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/LAMP_python3/tools/problems.py:14: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/LAMP_python3/tools/problems.py:46: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/LAMP_python3/tools/problems.py:46: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/LAMP_python3/tools/problems.py:47: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "Problem created ...\n",
            "A is:\n",
            "[[ 0.10273262 -0.03869087 -0.03340451 ... -0.14148578 -0.07005789\n",
            "  -0.0011206 ]\n",
            " [-0.10874406  0.00361265 -0.05056782 ... -0.00440344  0.02238073\n",
            "  -0.01182407]\n",
            " [-0.00969151 -0.15384534  0.03212775 ... -0.01327189  0.11998697\n",
            "  -0.08736685]\n",
            " ...\n",
            " [ 0.01206288 -0.10314706 -0.01349555 ... -0.08644557 -0.01524377\n",
            "   0.03501276]\n",
            " [ 0.10872197 -0.02853399 -0.0031452  ... -0.03708276 -0.00191837\n",
            "  -0.1022457 ]\n",
            " [ 0.22495401  0.04879155 -0.00785195 ... -0.0692498  -0.00567421\n",
            "   0.12222793]]\n",
            "theta_init=(1.0, 1.0)\n",
            "Building layers ... done\n",
            "WARNING:tensorflow:From /content/LAMP_python3/tools/train.py:78: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Plan the learning ... done\n",
            "Do the learning (takes a while)\n",
            "WARNING:tensorflow:From /content/LAMP_python3/tools/train.py:93: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/LAMP_python3/tools/train.py:94: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "norms xval:224.5516510 yval:224.0477230\n",
            "WARNING:tensorflow:From /content/LAMP_python3/tools/train.py:29: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "restoring B_0:0 is:[[ 0.08373718 -0.05146908 -0.25819403 ...  0.03072142  0.01907489\n",
            "   0.4177698 ]\n",
            " [-0.05983526  0.1724593  -0.3543346  ... -0.32865295 -0.05302119\n",
            "  -0.01978229]\n",
            " [-0.21183607 -0.020551    0.15397418 ...  0.06136413  0.04919515\n",
            "   0.31528714]\n",
            " ...\n",
            " [-0.25741753  0.02060023 -0.02623261 ... -0.11131408  0.01223096\n",
            "  -0.09451902]\n",
            " [-0.12830833  0.1594849   0.17236836 ... -0.1931393  -0.11318965\n",
            "   0.10567614]\n",
            " [ 0.10768005  0.06438731 -0.11200086 ...  0.17262119 -0.13797611\n",
            "   0.25149176]]\n",
            "WARNING:tensorflow:From /content/LAMP_python3/tools/train.py:33: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "restoring theta_0:0 is:[2.1251287 0.7605437]\n",
            "restoring theta_1:0 is:[2.1953428 1.2121087]\n",
            "restoring B_1:0 is:[[ 0.08549716 -0.0419279  -0.2078877  ...  0.0311866   0.00744562\n",
            "   0.34883413]\n",
            " [-0.03905236  0.11450251 -0.31370452 ... -0.2767972  -0.02654805\n",
            "  -0.03296836]\n",
            " [-0.19656593 -0.03280074  0.14309041 ...  0.05320174  0.01688856\n",
            "   0.30701292]\n",
            " ...\n",
            " [-0.22326277  0.01639407 -0.03554815 ... -0.10946281  0.01989694\n",
            "  -0.07619202]\n",
            " [-0.09826599  0.13152613  0.12201349 ... -0.17232342 -0.09203876\n",
            "   0.08225828]\n",
            " [ 0.10496026  0.05628599 -0.09193035 ...  0.14824742 -0.11436509\n",
            "   0.1919939 ]]\n",
            "restoring theta_2:0 is:[2.2537858 1.1882994]\n",
            "restoring B_2:0 is:[[ 0.10023646 -0.05161726 -0.20264135 ...  0.02949374  0.01422852\n",
            "   0.33848238]\n",
            " [-0.04166719  0.10189242 -0.30928677 ... -0.2755621  -0.02635179\n",
            "  -0.02971358]\n",
            " [-0.1733922  -0.02003503  0.14939602 ...  0.04424979  0.02703598\n",
            "   0.27064124]\n",
            " ...\n",
            " [-0.22643423  0.02691353 -0.04315814 ... -0.07860386  0.00806834\n",
            "  -0.06911639]\n",
            " [-0.11437359  0.14824693  0.12028288 ... -0.16245195 -0.1025978\n",
            "   0.08276053]\n",
            " [ 0.08413389  0.06608772 -0.08990993 ...  0.16205063 -0.10722753\n",
            "   0.20113541]]\n",
            "restoring theta_3:0 is:[2.365163  1.1576294]\n",
            "restoring B_3:0 is:[[ 0.09250023 -0.05898878 -0.18647884 ...  0.01396688  0.0092417\n",
            "   0.33130726]\n",
            " [-0.04108218  0.11387202 -0.27944863 ... -0.25383145 -0.04276388\n",
            "  -0.0165727 ]\n",
            " [-0.17802909 -0.03814918  0.14233778 ...  0.03474499  0.02131829\n",
            "   0.25206172]\n",
            " ...\n",
            " [-0.21819334  0.03016144 -0.02493952 ... -0.08711085  0.01450917\n",
            "  -0.07623731]\n",
            " [-0.10028245  0.14297481  0.11344678 ... -0.14792556 -0.10707622\n",
            "   0.07784826]\n",
            " [ 0.09037316  0.06079509 -0.10054889 ...  0.1303915  -0.11052462\n",
            "   0.18385214]]\n",
            "restoring theta_4:0 is:[2.4016192 1.1108059]\n",
            "restoring B_4:0 is:[[ 0.09773989 -0.06095095 -0.19574375 ...  0.01943358  0.01095024\n",
            "   0.3348063 ]\n",
            " [-0.04091313  0.10750743 -0.28503582 ... -0.2627526  -0.03602063\n",
            "  -0.01580225]\n",
            " [-0.17960441 -0.03731736  0.12608598 ...  0.02810156  0.02906112\n",
            "   0.24773315]\n",
            " ...\n",
            " [-0.2210038   0.02862747 -0.04243889 ... -0.08994316  0.02746263\n",
            "  -0.07629186]\n",
            " [-0.10080443  0.1463109   0.1270652  ... -0.1493147  -0.09618061\n",
            "   0.08375811]\n",
            " [ 0.07935442  0.0564016  -0.10081439 ...  0.1347488  -0.10869427\n",
            "   0.18892656]]\n",
            "restoring theta_5:0 is:[2.535786  1.0704328]\n",
            "restoring B_5:0 is:[[ 0.10613733 -0.0660254  -0.2002143  ...  0.01490797  0.01972872\n",
            "   0.3570912 ]\n",
            " [-0.03672657  0.11595065 -0.29248482 ... -0.2770657  -0.04094846\n",
            "  -0.01407653]\n",
            " [-0.18634397 -0.04841294  0.12530866 ...  0.03936459  0.02299725\n",
            "   0.24954204]\n",
            " ...\n",
            " [-0.21757229  0.02825573 -0.04114147 ... -0.08466764  0.02434572\n",
            "  -0.08098463]\n",
            " [-0.10555331  0.13520795  0.14389227 ... -0.15538317 -0.099114\n",
            "   0.07460192]\n",
            " [ 0.09391488  0.05501394 -0.09655688 ...  0.14809853 -0.10415635\n",
            "   0.18827744]]\n",
            "restoring theta_6:0 is:[2.1547737 1.0346749]\n",
            "restoring B_6:0 is:[[ 0.13490857 -0.12878557 -0.16442095 ...  0.03277892  0.06244064\n",
            "   0.36054707]\n",
            " [-0.06255859  0.10283997 -0.2725635  ... -0.24586898 -0.03672262\n",
            "   0.01382285]\n",
            " [-0.16847022 -0.06642129  0.11066782 ...  0.0183073   0.02414673\n",
            "   0.16928296]\n",
            " ...\n",
            " [-0.21016589  0.01289767 -0.05174062 ... -0.09649545  0.00522543\n",
            "  -0.09645584]\n",
            " [-0.08611494  0.11044876  0.18696639 ... -0.12642229 -0.07161025\n",
            "   0.03475183]\n",
            " [ 0.08943281  0.02827159 -0.09518941 ...  0.13277954 -0.10611002\n",
            "   0.1703157 ]]\n",
            "restoring theta_7:0 is:[1.4897784 1.01236  ]\n",
            "restoring B_7:0 is:[[ 0.09027603 -0.0923192  -0.06886563 ...  0.02371636  0.05629613\n",
            "   0.20762444]\n",
            " [-0.04134864  0.04403551 -0.15001746 ... -0.13248397 -0.02368308\n",
            "   0.01981905]\n",
            " [-0.08828998 -0.05185844  0.05677738 ...  0.00061768  0.01276933\n",
            "   0.06231739]\n",
            " ...\n",
            " [-0.11707984 -0.0004511  -0.02600056 ... -0.05391868 -0.00714892\n",
            "  -0.06092212]\n",
            " [-0.04183669  0.0501109   0.1221839  ... -0.05780397 -0.02961772\n",
            "  -0.00024779]\n",
            " [ 0.04204961  0.00293151 -0.0554188  ...  0.07365352 -0.05974756\n",
            "   0.09238607]]\n",
            "Already did Linear trainrate=0.5. Skipping.\n",
            "Already did Linear trainrate=0.1. Skipping.\n",
            "Already did Linear trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft T=1. Skipping.\n",
            "Already did LAMP-soft T=1 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft T=1 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft T=1 trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft linear T=2. Skipping.\n",
            "Already did LAMP-soft linear T=2 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft linear T=2 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft linear T=2 trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft non-linear T=2. Skipping.\n",
            "Already did LAMP-soft non-linear T=2 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft non-linear T=2 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft non-linear T=2 trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft linear T=3. Skipping.\n",
            "Already did LAMP-soft linear T=3 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft linear T=3 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft linear T=3 trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft non-linear T=3. Skipping.\n",
            "Already did LAMP-soft non-linear T=3 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft non-linear T=3 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft non-linear T=3 trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft linear T=4. Skipping.\n",
            "Already did LAMP-soft linear T=4 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft linear T=4 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft linear T=4 trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft non-linear T=4. Skipping.\n",
            "Already did LAMP-soft non-linear T=4 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft non-linear T=4 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft non-linear T=4 trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft linear T=5. Skipping.\n",
            "Already did LAMP-soft linear T=5 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft linear T=5 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft linear T=5 trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft non-linear T=5. Skipping.\n",
            "Already did LAMP-soft non-linear T=5 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft non-linear T=5 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft non-linear T=5 trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft linear T=6. Skipping.\n",
            "Already did LAMP-soft linear T=6 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft linear T=6 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft linear T=6 trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft non-linear T=6. Skipping.\n",
            "Already did LAMP-soft non-linear T=6 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft non-linear T=6 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft non-linear T=6 trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft linear T=7. Skipping.\n",
            "Already did LAMP-soft linear T=7 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft linear T=7 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft linear T=7 trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft non-linear T=7. Skipping.\n",
            "Already did LAMP-soft non-linear T=7 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft non-linear T=7 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft non-linear T=7 trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft linear T=8. Skipping.\n",
            "Already did LAMP-soft linear T=8 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft linear T=8 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft linear T=8 trainrate=0.01. Skipping.\n",
            "Already did LAMP-soft non-linear T=8. Skipping.\n",
            "Already did LAMP-soft non-linear T=8 trainrate=0.5. Skipping.\n",
            "Already did LAMP-soft non-linear T=8 trainrate=0.1. Skipping.\n",
            "Already did LAMP-soft non-linear T=8 trainrate=0.01. Skipping.\n",
            "LAMP-soft T=1 nmse=-6.218525 dB\n",
            "LAMP-soft linear T=2 nmse=-1.558928 dB\n",
            "LAMP-soft non-linear T=2 nmse=-11.149492 dB\n",
            "LAMP-soft linear T=3 nmse=-6.030330 dB\n",
            "LAMP-soft non-linear T=3 nmse=-15.739241 dB\n",
            "LAMP-soft linear T=4 nmse=-11.111426 dB\n",
            "LAMP-soft non-linear T=4 nmse=-20.009360 dB\n",
            "LAMP-soft linear T=5 nmse=-15.367776 dB\n",
            "LAMP-soft non-linear T=5 nmse=-24.644411 dB\n",
            "LAMP-soft linear T=6 nmse=-18.936634 dB\n",
            "LAMP-soft non-linear T=6 nmse=-29.489040 dB\n",
            "LAMP-soft linear T=7 nmse=-22.657232 dB\n",
            "LAMP-soft non-linear T=7 nmse=-33.414526 dB\n",
            "LAMP-soft linear T=8 nmse=-30.440512 dB\n",
            "LAMP-soft non-linear T=8 nmse=-37.886918 dB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_JUJ_X0d0rw",
        "colab_type": "code",
        "outputId": "88c48794-b183-4d16-b51f-8d817ae211f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "# You can evaluate network agian to make sure that restored parameters and matrices give nmse as before\n",
        "print('Evaluating the learned network ...')\n",
        "train.evaluate_nmse(sess, training_stages, prob, 'drive/My Drive/LAMP_bg_giid.npz' )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating the learned network ...\n",
            "LAMP-soft T=1 nmse=-6.452799 dB\n",
            "LAMP-soft linear T=2 nmse=-1.771318 dB\n",
            "LAMP-soft non-linear T=2 nmse=-11.257845 dB\n",
            "LAMP-soft linear T=3 nmse=-6.264145 dB\n",
            "LAMP-soft non-linear T=3 nmse=-15.935028 dB\n",
            "LAMP-soft linear T=4 nmse=-11.320431 dB\n",
            "LAMP-soft non-linear T=4 nmse=-20.310414 dB\n",
            "LAMP-soft linear T=5 nmse=-15.656275 dB\n",
            "LAMP-soft non-linear T=5 nmse=-24.909551 dB\n",
            "LAMP-soft linear T=6 nmse=-19.437523 dB\n",
            "LAMP-soft non-linear T=6 nmse=-29.503319 dB\n",
            "LAMP-soft linear T=7 nmse=-22.995415 dB\n",
            "LAMP-soft non-linear T=7 nmse=-33.072743 dB\n",
            "LAMP-soft linear T=8 nmse=-30.646625 dB\n",
            "LAMP-soft non-linear T=8 nmse=-37.773759 dB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hq1zy1vRvPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we import the necessary libraries and modules \n",
        "#!/usr/bin/python\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\"\"\"\n",
        "This file serves as an example of how to \n",
        "a) select a problem to be solved \n",
        "b) select a network type\n",
        "c) train the network to minimize recovery MSE\n",
        "\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # BE QUIET!!!!\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(1) # numpy is good about making repeatable output\n",
        "tf.set_random_seed(1) # on the other hand, this is basically useless (see issue 9171)\n",
        "\n",
        "# import our problems, networks and training modules\n",
        "from tools import problems,networks,train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kto7PquoRxuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I use this line to get the measurement matrix which is hidden in prob variable\n",
        "prob = problems.bernoulli_gaussian_trial(kappa=None,M=250,N=500,L=1000,pnz=.1,SNR=40) #a Bernoulli-Gaussian x, noisily observed through a random matrix\n",
        "A = prob.A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_7JqoD6R9qC",
        "colab_type": "code",
        "outputId": "d15dcff6-be6a-4402-d58e-ee38d2336279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Insted of reading the matrices B_t from the netowrk, I rather read them from the file\n",
        "filename = 'drive/My Drive/LAMP_bg_giid.npz'\n",
        "\n",
        "B_t = []\n",
        "theta_t = []\n",
        "\n",
        "try:\n",
        "    filecontent = np.load(filename).items()\n",
        "    for k, d in filecontent:\n",
        "        if k.startswith('B_'):\n",
        "            B_t.append(d)\n",
        "            print('reading ' + k + ' as:' + str(d))\n",
        "        elif k.startswith('theta_'):\n",
        "            theta_t.append(d)\n",
        "            print('reading ' + k + ' as:' + str(d))\n",
        "except IOError:\n",
        "    print('Couldnt read the file')\n",
        "    pass"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading B_0:0 as:[[ 0.08373718 -0.05146908 -0.25819403 ...  0.03072142  0.01907489\n",
            "   0.4177698 ]\n",
            " [-0.05983526  0.1724593  -0.3543346  ... -0.32865295 -0.05302119\n",
            "  -0.01978229]\n",
            " [-0.21183607 -0.020551    0.15397418 ...  0.06136413  0.04919515\n",
            "   0.31528714]\n",
            " ...\n",
            " [-0.25741753  0.02060023 -0.02623261 ... -0.11131408  0.01223096\n",
            "  -0.09451902]\n",
            " [-0.12830833  0.1594849   0.17236836 ... -0.1931393  -0.11318965\n",
            "   0.10567614]\n",
            " [ 0.10768005  0.06438731 -0.11200086 ...  0.17262119 -0.13797611\n",
            "   0.25149176]]\n",
            "reading theta_0:0 as:[2.1251287 0.7605437]\n",
            "reading theta_1:0 as:[2.1953428 1.2121087]\n",
            "reading B_1:0 as:[[ 0.08549716 -0.0419279  -0.2078877  ...  0.0311866   0.00744562\n",
            "   0.34883413]\n",
            " [-0.03905236  0.11450251 -0.31370452 ... -0.2767972  -0.02654805\n",
            "  -0.03296836]\n",
            " [-0.19656593 -0.03280074  0.14309041 ...  0.05320174  0.01688856\n",
            "   0.30701292]\n",
            " ...\n",
            " [-0.22326277  0.01639407 -0.03554815 ... -0.10946281  0.01989694\n",
            "  -0.07619202]\n",
            " [-0.09826599  0.13152613  0.12201349 ... -0.17232342 -0.09203876\n",
            "   0.08225828]\n",
            " [ 0.10496026  0.05628599 -0.09193035 ...  0.14824742 -0.11436509\n",
            "   0.1919939 ]]\n",
            "reading theta_2:0 as:[2.2537858 1.1882994]\n",
            "reading B_2:0 as:[[ 0.10023646 -0.05161726 -0.20264135 ...  0.02949374  0.01422852\n",
            "   0.33848238]\n",
            " [-0.04166719  0.10189242 -0.30928677 ... -0.2755621  -0.02635179\n",
            "  -0.02971358]\n",
            " [-0.1733922  -0.02003503  0.14939602 ...  0.04424979  0.02703598\n",
            "   0.27064124]\n",
            " ...\n",
            " [-0.22643423  0.02691353 -0.04315814 ... -0.07860386  0.00806834\n",
            "  -0.06911639]\n",
            " [-0.11437359  0.14824693  0.12028288 ... -0.16245195 -0.1025978\n",
            "   0.08276053]\n",
            " [ 0.08413389  0.06608772 -0.08990993 ...  0.16205063 -0.10722753\n",
            "   0.20113541]]\n",
            "reading theta_3:0 as:[2.365163  1.1576294]\n",
            "reading B_3:0 as:[[ 0.09250023 -0.05898878 -0.18647884 ...  0.01396688  0.0092417\n",
            "   0.33130726]\n",
            " [-0.04108218  0.11387202 -0.27944863 ... -0.25383145 -0.04276388\n",
            "  -0.0165727 ]\n",
            " [-0.17802909 -0.03814918  0.14233778 ...  0.03474499  0.02131829\n",
            "   0.25206172]\n",
            " ...\n",
            " [-0.21819334  0.03016144 -0.02493952 ... -0.08711085  0.01450917\n",
            "  -0.07623731]\n",
            " [-0.10028245  0.14297481  0.11344678 ... -0.14792556 -0.10707622\n",
            "   0.07784826]\n",
            " [ 0.09037316  0.06079509 -0.10054889 ...  0.1303915  -0.11052462\n",
            "   0.18385214]]\n",
            "reading theta_4:0 as:[2.4016192 1.1108059]\n",
            "reading B_4:0 as:[[ 0.09773989 -0.06095095 -0.19574375 ...  0.01943358  0.01095024\n",
            "   0.3348063 ]\n",
            " [-0.04091313  0.10750743 -0.28503582 ... -0.2627526  -0.03602063\n",
            "  -0.01580225]\n",
            " [-0.17960441 -0.03731736  0.12608598 ...  0.02810156  0.02906112\n",
            "   0.24773315]\n",
            " ...\n",
            " [-0.2210038   0.02862747 -0.04243889 ... -0.08994316  0.02746263\n",
            "  -0.07629186]\n",
            " [-0.10080443  0.1463109   0.1270652  ... -0.1493147  -0.09618061\n",
            "   0.08375811]\n",
            " [ 0.07935442  0.0564016  -0.10081439 ...  0.1347488  -0.10869427\n",
            "   0.18892656]]\n",
            "reading theta_5:0 as:[2.535786  1.0704328]\n",
            "reading B_5:0 as:[[ 0.10613733 -0.0660254  -0.2002143  ...  0.01490797  0.01972872\n",
            "   0.3570912 ]\n",
            " [-0.03672657  0.11595065 -0.29248482 ... -0.2770657  -0.04094846\n",
            "  -0.01407653]\n",
            " [-0.18634397 -0.04841294  0.12530866 ...  0.03936459  0.02299725\n",
            "   0.24954204]\n",
            " ...\n",
            " [-0.21757229  0.02825573 -0.04114147 ... -0.08466764  0.02434572\n",
            "  -0.08098463]\n",
            " [-0.10555331  0.13520795  0.14389227 ... -0.15538317 -0.099114\n",
            "   0.07460192]\n",
            " [ 0.09391488  0.05501394 -0.09655688 ...  0.14809853 -0.10415635\n",
            "   0.18827744]]\n",
            "reading theta_6:0 as:[2.1547737 1.0346749]\n",
            "reading B_6:0 as:[[ 0.13490857 -0.12878557 -0.16442095 ...  0.03277892  0.06244064\n",
            "   0.36054707]\n",
            " [-0.06255859  0.10283997 -0.2725635  ... -0.24586898 -0.03672262\n",
            "   0.01382285]\n",
            " [-0.16847022 -0.06642129  0.11066782 ...  0.0183073   0.02414673\n",
            "   0.16928296]\n",
            " ...\n",
            " [-0.21016589  0.01289767 -0.05174062 ... -0.09649545  0.00522543\n",
            "  -0.09645584]\n",
            " [-0.08611494  0.11044876  0.18696639 ... -0.12642229 -0.07161025\n",
            "   0.03475183]\n",
            " [ 0.08943281  0.02827159 -0.09518941 ...  0.13277954 -0.10611002\n",
            "   0.1703157 ]]\n",
            "reading theta_7:0 as:[1.4897784 1.01236  ]\n",
            "reading B_7:0 as:[[ 0.09027603 -0.0923192  -0.06886563 ...  0.02371636  0.05629613\n",
            "   0.20762444]\n",
            " [-0.04134864  0.04403551 -0.15001746 ... -0.13248397 -0.02368308\n",
            "   0.01981905]\n",
            " [-0.08828998 -0.05185844  0.05677738 ...  0.00061768  0.01276933\n",
            "   0.06231739]\n",
            " ...\n",
            " [-0.11707984 -0.0004511  -0.02600056 ... -0.05391868 -0.00714892\n",
            "  -0.06092212]\n",
            " [-0.04183669  0.0501109   0.1221839  ... -0.05780397 -0.02961772\n",
            "  -0.00024779]\n",
            " [ 0.04204961  0.00293151 -0.0554188  ...  0.07365352 -0.05974756\n",
            "   0.09238607]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWATBQ8wSCus",
        "colab_type": "code",
        "outputId": "f78c4015-b57b-479f-854f-ef377ae56c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Do SVD for A and B_t\n",
        "B_t = np.asarray(B_t)\n",
        "\n",
        "# Shape of B_t should be 8x500x250\n",
        "print(B_t.shape)\n",
        "\n",
        "M,N = A.shape\n",
        "U_A, S_A, Vh_A = np.linalg.svd(A.transpose())\n",
        "\n",
        "U_t = np.empty([8,500,500])\n",
        "S_t = np.empty([8,250])\n",
        "V_t = np.empty([8,250,250])\n",
        "\n",
        "for i in range(len(B_t)):\n",
        "    U, S, Vh = np.linalg.svd(B_t[i])\n",
        "    U_t[i] = U\n",
        "    S_t[i] = S\n",
        "    V_t[i] = Vh"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 500, 250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17q_kCJJSG3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## A few test to check if the SVD of A makes sense\n",
        "\n",
        "# Check if U_AˆT*U_A is a unitary matrix with dimensions 250x250\n",
        "# Here comes your code ...\n",
        "# Check if Vh_AˆT*Vh_A is a unitary matrix with dimensions 500x500\n",
        "# Here comes your code ...\n",
        "\n",
        "# Print out Frobenius norm of A\n",
        "# Here comes your code ...\n",
        "\n",
        "# Plot singular values of A\n",
        "# Here comes your code ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sik19i_zSLPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Frobenius norms of B_t\n",
        "\n",
        "# Plot Frobenius norms of B_t against t\n",
        "# Here comes your code ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUUnYzrp3N40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## How close is B_t to AˆT?\n",
        "\n",
        "# Plot Frobenius norms of B_t-AˆT against t\n",
        "# Here comes your code ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv2EKdFOirJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## How close is B_t to mu_t * AˆT?\n",
        "\n",
        "# For each t, find mu_t which minimizes Frobenius norm of B_t-mu_t*A\n",
        "# Here comes your code ...\n",
        "\n",
        "# ---- Plotting results ----\n",
        "# Plot Frobenius norms of B_t-mu_t*AˆT against t\n",
        "# On the same figure, plot B_t-AˆT against t\n",
        "# Add figure title, legend, axis labels\n",
        "# Here comes your code ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK9-qpQvt0Za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Are succesive B_t getting closer to each other\n",
        "\n",
        "# Plot Frobenius norms of Frobenius norms of B_{t+1}-B_t against t\n",
        "# Here comes your code ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJulyp_P-Eor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Plotting the parameters of the soft thresholder\n",
        "\n",
        "# Plot theta_t (2 parameters) against t\n",
        "# Add figure title, legend, axis labels\n",
        "# Here comes your code ...\n",
        "\n",
        "# Is scaling parameter significantely changing across iterations? "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_efF2hb-_sCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Plotting U_t matrices\n",
        "\n",
        "# Plot U_t matrices as gray scale images\n",
        "# Here comes your code ... "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvCnCFLY9Ewg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Plotting U_tˆT*U_k. White is 1, black is -1, and gray is 0 \n",
        "\n",
        "# For each matrix U_t plot U_tˆT*U_k. White is 1, black is -1, and gray is 0\n",
        "# Here comes your code ... \n",
        "      \n",
        "# Can we conclude the matrices have roughly 'the same' left signal spaces?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFIgPNs1Xpeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Plotting V_tˆT*V_k. White is 1, black is -1, and gray is 0 \n",
        "\n",
        "# For each matrix V_t plot V_tˆT*V_k. White is 1, black is -1, and gray is 0\n",
        "# Here comes your code ... \n",
        "\n",
        "# Can we draw any conclusions from here?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD6DF8IHa2dx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}