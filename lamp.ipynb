{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0924 15:24:02.904218 140410043807552 deprecation_wrapper.py:119] From /home/jungp/Projects/DAAD-AIMS/lamp/LAMP_python3/LAMP.py:19: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "W0924 15:24:02.952244 140410043807552 deprecation_wrapper.py:119] From /home/jungp/Projects/DAAD-AIMS/lamp/LAMP_python3/tools/problems.py:14: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0924 15:24:02.958547 140410043807552 deprecation_wrapper.py:119] From /home/jungp/Projects/DAAD-AIMS/lamp/LAMP_python3/tools/problems.py:47: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0924 15:24:02.977124 140410043807552 deprecation.py:323] From /home/jungp/Projects/DAAD-AIMS/lamp/LAMP_python3/tools/problems.py:47: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0924 15:24:02.980888 140410043807552 deprecation_wrapper.py:119] From /home/jungp/Projects/DAAD-AIMS/lamp/LAMP_python3/tools/problems.py:48: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The measurement matrix A is: [[ 0.10273262 -0.03869087 -0.03340451 ... -0.14148578 -0.07005789\n",
      "  -0.0011206 ]\n",
      " [-0.10874406  0.00361265 -0.05056782 ... -0.00440344  0.02238073\n",
      "  -0.01182407]\n",
      " [-0.00969151 -0.15384534  0.03212775 ... -0.01327189  0.11998697\n",
      "  -0.08736685]\n",
      " ...\n",
      " [ 0.01206288 -0.10314706 -0.01349555 ... -0.08644557 -0.01524377\n",
      "   0.03501276]\n",
      " [ 0.10872197 -0.02853399 -0.0031452  ... -0.03708276 -0.00191837\n",
      "  -0.1022457 ]\n",
      " [ 0.22495401  0.04879155 -0.00785195 ... -0.0692498  -0.00567421\n",
      "   0.12222793]]\n",
      "Problem created ...\n",
      "A is:\n",
      "[[ 0.10273262 -0.03869087 -0.03340451 ... -0.14148578 -0.07005789\n",
      "  -0.0011206 ]\n",
      " [-0.10874406  0.00361265 -0.05056782 ... -0.00440344  0.02238073\n",
      "  -0.01182407]\n",
      " [-0.00969151 -0.15384534  0.03212775 ... -0.01327189  0.11998697\n",
      "  -0.08736685]\n",
      " ...\n",
      " [ 0.01206288 -0.10314706 -0.01349555 ... -0.08644557 -0.01524377\n",
      "   0.03501276]\n",
      " [ 0.10872197 -0.02853399 -0.0031452  ... -0.03708276 -0.00191837\n",
      "  -0.1022457 ]\n",
      " [ 0.22495401  0.04879155 -0.00785195 ... -0.0692498  -0.00567421\n",
      "   0.12222793]]\n",
      "theta_init=(1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0924 15:24:03.597681 140410043807552 deprecation_wrapper.py:119] From /home/jungp/Projects/DAAD-AIMS/lamp/LAMP_python3/tools/train.py:66: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building layers ... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0924 15:24:03.870146 140410043807552 deprecation.py:323] From /products/anaconda/new/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0924 15:24:18.604837 140410043807552 deprecation_wrapper.py:119] From /home/jungp/Projects/DAAD-AIMS/lamp/LAMP_python3/tools/train.py:81: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan the learning ... done\n",
      "Do the learning (takes a while)\n",
      "norms xval:224.5516510 yval:224.0477230\n",
      "restoring B_0:0 is:[[ 0.08992807 -0.06222115 -0.21496771 ...  0.01664005  0.01437154\n",
      "   0.33871213]\n",
      " [-0.03103256  0.12396946 -0.29553628 ... -0.27324465 -0.0478969\n",
      "  -0.02244472]\n",
      " [-0.184345   -0.02802262  0.11586176 ...  0.03836976  0.01886664\n",
      "   0.27416116]\n",
      " ...\n",
      " [-0.20614995  0.02505352 -0.02856944 ... -0.08639485  0.01625221\n",
      "  -0.06706633]\n",
      " [-0.09438569  0.14654519  0.11937334 ... -0.15582246 -0.10051332\n",
      "   0.07352506]\n",
      " [ 0.0832004   0.06218934 -0.09735402 ...  0.13796064 -0.11874982\n",
      "   0.18497868]]\n",
      "restoring theta_0:0 is:[1.7374297  0.97038484]\n",
      "restoring theta_1:0 is:[2.132359  1.0997802]\n",
      "restoring theta_2:0 is:[2.229824  1.1538923]\n",
      "restoring theta_3:0 is:[2.3242338 1.1054397]\n",
      "restoring theta_4:0 is:[2.2436528 1.0658582]\n",
      "restoring theta_5:0 is:[2.1434922 1.0350698]\n",
      "Already did Linear trainrate=0.5. Skipping.\n",
      "Already did Linear trainrate=0.1. Skipping.\n",
      "Already did Linear trainrate=0.01. Skipping.\n",
      "Already did LAMP-soft T=1. Skipping.\n",
      "Already did LAMP-soft T=1 trainrate=0.5. Skipping.\n",
      "Already did LAMP-soft T=1 trainrate=0.1. Skipping.\n",
      "Already did LAMP-soft T=1 trainrate=0.01. Skipping.\n",
      "Already did LAMP-soft non-linear T=2. Skipping.\n",
      "Already did LAMP-soft non-linear T=2 trainrate=0.5. Skipping.\n",
      "Already did LAMP-soft non-linear T=2 trainrate=0.1. Skipping.\n",
      "Already did LAMP-soft non-linear T=2 trainrate=0.01. Skipping.\n",
      "Already did LAMP-soft non-linear T=3. Skipping.\n",
      "Already did LAMP-soft non-linear T=3 trainrate=0.5. Skipping.\n",
      "Already did LAMP-soft non-linear T=3 trainrate=0.1. Skipping.\n",
      "Already did LAMP-soft non-linear T=3 trainrate=0.01. Skipping.\n",
      "Already did LAMP-soft non-linear T=4. Skipping.\n",
      "Already did LAMP-soft non-linear T=4 trainrate=0.5. Skipping.\n",
      "Already did LAMP-soft non-linear T=4 trainrate=0.1. Skipping.\n",
      "Already did LAMP-soft non-linear T=4 trainrate=0.01. Skipping.\n",
      "Already did LAMP-soft non-linear T=5. Skipping.\n",
      "Already did LAMP-soft non-linear T=5 trainrate=0.5. Skipping.\n",
      "Already did LAMP-soft non-linear T=5 trainrate=0.1. Skipping.\n",
      "Already did LAMP-soft non-linear T=5 trainrate=0.01. Skipping.\n",
      "Already did LAMP-soft non-linear T=6. Skipping.\n",
      "Already did LAMP-soft non-linear T=6 trainrate=0.5. Skipping.\n",
      "Already did LAMP-soft non-linear T=6 trainrate=0.1. Skipping.\n",
      "Already did LAMP-soft non-linear T=6 trainrate=0.01. Skipping.\n",
      "LAMP-soft T=1 nmse=-6.204981 dB\n",
      "LAMP-soft non-linear T=2 nmse=-10.810832 dB\n",
      "LAMP-soft non-linear T=3 nmse=-14.440235 dB\n",
      "LAMP-soft non-linear T=4 nmse=-16.597939 dB\n",
      "LAMP-soft non-linear T=5 nmse=-17.065920 dB\n",
      "LAMP-soft non-linear T=6 nmse=-16.783185 dB\n"
     ]
    }
   ],
   "source": [
    "%run 'LAMP.py'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ad713558b836453a920856e7f99fa8ee",
   "lastKernelId": "786cb5cb-c237-41b8-a3b0-88d4e3297915"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
